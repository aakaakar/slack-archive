[
    {
        "type": "message",
        "user": "U0UGH3X7X",
        "text": "scrum ...",
        "ts": "1481554841.000524"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Link?",
        "ts": "1481554881.000525"
    },
    {
        "text": "<@U0UGH3X7X|tuand> has started a Google+ Hangout for this channel. <https:\/\/hangouts.google.com\/hangouts\/_\/2fjvq6xp6vf37g3fo3sonolvtme|Join Hangout>.",
        "username": "hangouts",
        "bot_id": "B0UKUAJ0Y",
        "type": "message",
        "subtype": "bot_message",
        "ts": "1481554885.000526"
    },
    {
        "type": "message",
        "subtype": "file_share",
        "text": "<@U0N1D1UAE|bcbrock> uploaded a file: <https:\/\/hyperledgerproject.slack.com\/files\/bcbrock\/F3DRVNT34\/screen_shot_2016-12-12_at_4.32.25_pm.png|Screen Shot 2016-12-12 at 4.32.25 PM.png>",
        "file": {
            "id": "F3DRVNT34",
            "created": 1481582034,
            "timestamp": 1481582034,
            "name": "Screen Shot 2016-12-12 at 4.32.25 PM.png",
            "title": "Screen Shot 2016-12-12 at 4.32.25 PM.png",
            "mimetype": "image\/png",
            "filetype": "png",
            "pretty_type": "PNG",
            "user": "U0N1D1UAE",
            "editable": false,
            "size": 342959,
            "mode": "hosted",
            "is_external": false,
            "external_type": "",
            "is_public": true,
            "public_url_shared": false,
            "display_as_bot": false,
            "username": "",
            "url_private": "https:\/\/files.slack.com\/files-pri\/T0J024XGA-F3DRVNT34\/screen_shot_2016-12-12_at_4.32.25_pm.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "url_private_download": "https:\/\/files.slack.com\/files-pri\/T0J024XGA-F3DRVNT34\/download\/screen_shot_2016-12-12_at_4.32.25_pm.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_64": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_64.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_80": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_80.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_360": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_360.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_360_w": 360,
            "thumb_360_h": 161,
            "thumb_480": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_480.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_480_w": 480,
            "thumb_480_h": 215,
            "thumb_160": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_160.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_720": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_720.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_720_w": 720,
            "thumb_720_h": 322,
            "thumb_960": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_960.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_960_w": 960,
            "thumb_960_h": 430,
            "thumb_1024": "https:\/\/files.slack.com\/files-tmb\/T0J024XGA-F3DRVNT34-b02de37985\/screen_shot_2016-12-12_at_4.32.25_pm_1024.png?t=xoxe-18002167554-139099126023-137701436192-e599afc92e",
            "thumb_1024_w": 1024,
            "thumb_1024_h": 458,
            "image_exif_rotation": 1,
            "original_w": 1890,
            "original_h": 846,
            "permalink": "https:\/\/hyperledgerproject.slack.com\/files\/bcbrock\/F3DRVNT34\/screen_shot_2016-12-12_at_4.32.25_pm.png",
            "permalink_public": "https:\/\/slack-files.com\/T0J024XGA-F3DRVNT34-afa208e1e7",
            "channels": [
                "C0Z4NBUN6"
            ],
            "groups": [],
            "ims": [],
            "comments_count": 0
        },
        "user": "U0N1D1UAE",
        "upload": true,
        "display_as_bot": false,
        "username": "<@U0N1D1UAE|bcbrock>",
        "bot_id": null,
        "ts": "1481582037.000527"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "So, Bishop's graph above shows the flow control that is performed by HTTP\/2 (<https:\/\/http2.github.io\/http2-spec\/#FlowControl>) when pushing messages down a system that performs no flow control on its own (other than to block when its queue is full, as is the case with the Kafka orderer). Notice how the latency (between transmission and reception) settles to a steady state after a while.",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1481583678.000000"
        },
        "ts": "1481583673.000528"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "The argument here is that since flow control is being handled at the underlying layer, we should not worry about it on the application layer. This is w\/r\/t to the work that's being done on the broadcasting side to fix some of the issues on the common component.",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1481583768.000000"
        },
        "ts": "1481583690.000531"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "(<@U0N1D1UAE> please correct me if I'm misrepresenting things here.)",
        "ts": "1481583958.000535"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "I'm thinking about this and I think I'm sold. Given the graph above, do we still have good reasons for wanting to do flow control on the app layer? I may be missing something. ",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1481585993.000000"
        },
        "ts": "1481584034.000536"
    },
    {
        "type": "message",
        "user": "U0N1D1UAE",
        "text": "One other piece of info. is needed for the proof: In this run, 200K TX were broadcast and delivered. It took between 35 and 40 seconds. If the broadcast queue is of size 200K, the broadcast of 200K TX will complete in 10 seconds. Also note that broadcast and deliver clients are separate, completely independent processes. This and the above show that the underlying protocols are handling the flow control.",
        "ts": "1481584136.000538"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "if I am reading well the same would apply to (s)bft?",
        "ts": "1481588450.000540"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "I am asking in the context of <@U0XRC0KLH> out-to-lunch tests :wink: <https:\/\/github.com\/bft-smart\/library\/issues\/27>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "DefaultRecoverable doesn&#39;t seem to deal with nodes going out-to-lunch and then returning \u00b7 Issue #27 \u00b7 bft-smart\/library \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/bft-smart\/library\/issues\/27",
                "text": "I've written a simple test application with BFT-Smart, as a warmup for doing something real. This application consists in: (a) a simple client that sends \"commands\" consisting of some fixed amount...",
                "fallback": "GitHub: DefaultRecoverable doesn't seem to deal with nodes going out-to-lunch and then returning \u00b7 Issue #27 \u00b7 bft-smart\/library",
                "thumb_url": "https:\/\/avatars0.githubusercontent.com\/u\/1755771?v=3&s=400",
                "from_url": "https:\/\/github.com\/bft-smart\/library\/issues\/27",
                "thumb_width": 420,
                "thumb_height": 420,
                "service_icon": "https:\/\/a.slack-edge.com\/bfaba\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1481588639.000541"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "<@U0XQ35CDD> <@U0N1D1UAE> ^^",
        "ts": "1481588764.000543"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "can a Byz HTTP\/2 sender somehow circumvent this flow control?",
        "ts": "1481588840.000544"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "(if so this attack would be relevant to Kafka as well)",
        "ts": "1481588986.000545"
    },
    {
        "type": "message",
        "user": "U0N1D1UAE",
        "text": "I assume a malicious implementation of GPRC could ignore the HTTP\/2 flow control. I don\u2019t know what would happen in that case.",
        "ts": "1481589130.000546"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Good point. The only reference to this on the HTTP\/2 spec is on Section 10.5 (talking about the abuse of WINDOW_UPDATE). Based on what I'm reading here <https:\/\/www.imperva.com\/docs\/Imperva_HII_HTTP2.pdf>, it comes down to whether the HTTP\/2 server is implemented in a way that addresses this concern.",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1481595282.000000"
        },
        "ts": "1481589562.000547"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "&gt; In at least two cases we found HTTP\/2 implementations that specifically failed to account for the typical traps designers warn about. See Flow Control DoS, Dependency Cycle. One example is an attack based on abuse of the flow control WINDOW_UPDATE for DoS attacks, which was specifically warned about...",
        "ts": "1481589582.000548"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "So then the question is whether gRPC addresses that.",
        "ts": "1481589596.000549"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "(And I can see an argument in favor of flow control in the app layer being that we don't want to be married to how the underlying framework servers HTTP\/2 requests, but I would argue that whatever framework we choose _should_ address this.)",
        "ts": "1481589726.000550"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Anyway, I'll look into gRPC.",
        "ts": "1481589736.000551"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "thks",
        "ts": "1481589806.000552"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0N1D1UAE> <@U0XQ35CDD> Which flow control mechanism was this, is this pre or post <https:\/\/gerrit.hyperledger.org\/r\/#\/c\/3185\/> ?",
        "ts": "1481590353.000553"
    },
    {
        "type": "message",
        "user": "U0N1D1UAE",
        "text": "<@U0XPR4NP4> I am trying to argue that neither the broadcast nor deliver protocols require explicit flow control (windows). The experiment above was using Kosta\u2019s code <https:\/\/github.com\/kchristidis\/fabric\/tree\/fab-819-preview>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "kchristidis\/fabric",
                "title_link": "https:\/\/github.com\/kchristidis\/fabric\/tree\/fab-819-preview",
                "text": "Blockchain fabric code",
                "fallback": "GitHub: kchristidis\/fabric",
                "thumb_url": "https:\/\/avatars2.githubusercontent.com\/u\/14876848?v=3&s=400",
                "from_url": "https:\/\/github.com\/kchristidis\/fabric\/tree\/fab-819-preview",
                "thumb_width": 400,
                "thumb_height": 400,
                "service_icon": "https:\/\/github.com\/apple-touch-icon.png",
                "id": 1
            }
        ],
        "ts": "1481591761.000554"
    },
    {
        "type": "message",
        "user": "U0N1D1UAE",
        "text": "In the experiment the flow control that was eliminated was here: <https:\/\/github.com\/kchristidis\/fabric\/blob\/fab-819-preview\/orderer\/common\/broadcast\/broadcast.go#L142>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "kchristidis\/fabric",
                "title_link": "https:\/\/github.com\/kchristidis\/fabric\/blob\/fab-819-preview\/orderer\/common\/broadcast\/broadcast.go#L142",
                "text": "Blockchain fabric code",
                "fallback": "GitHub: kchristidis\/fabric",
                "thumb_url": "https:\/\/avatars2.githubusercontent.com\/u\/14876848?v=3&s=400",
                "from_url": "https:\/\/github.com\/kchristidis\/fabric\/blob\/fab-819-preview\/orderer\/common\/broadcast\/broadcast.go#L142",
                "thumb_width": 400,
                "thumb_height": 400,
                "service_icon": "https:\/\/github.com\/apple-touch-icon.png",
                "id": 1
            }
        ],
        "ts": "1481591849.000556"
    },
    {
        "type": "message",
        "user": "U0N1D1UAE",
        "text": "Where the broadcast server sends an error response if it\u2019s queue overflows. Instead, we simply let the broadcast server block.",
        "ts": "1481591908.000558"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "(<@U0XPR4NP4>: The point here being that the work in 3185 may not be necessary, and instead we move back to a simpler model, similar to the one that I wrote for the Kafka orderer originally, pre- common components)",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1481600359.000000"
        },
        "ts": "1481592467.000559"
    },
    {
        "type": "message",
        "user": "U0N1D1UAE",
        "text": "<@U0XQ35CDD> My reading of the Go grpc code is that it does check the window limits, and will close the connection or stream as appropriate according to which window size was violated. I\u2019ve hacked the http2_client to behave badly and gotten the http2:ErrCodeFlowControl == codes.ResourceExhausted error (although it did not manifest how I might have expected, so I may not have hacked it correctly.)",
        "ts": "1481593750.000565"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Ah, excellent. I also opened an issue on the gRPC-go repo to get confirmation of this behavior.",
        "ts": "1481595503.000568"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0N1D1UAE> <@U0XQ35CDD> Sounds promising.  Seems like it might be a clear win for broadcast, though would like to think on this a little more.  On deliver, getting rid of the window size seems a little more problematic without other API changes, as the window size allows a client to do something like \"retrieve blocks 3-7\" without the server attempting to deliver blocks \"3 until the gRPC buffer fills up\".  Maybe the right answer is to just modify the API to more explicitly support this though.",
        "ts": "1481598961.000569"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Since this buffer is at the HTTP2 layer, I assume that the buffer is shared for the whole stream? IE, 10x the broadcast calls does not give you 10x the window (which sounds like yet another advantage.)",
        "edited": {
            "user": "U0XPR4NP4",
            "ts": "1481599192.000000"
        },
        "ts": "1481599186.000570"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Okay, my gut reaction is to kill windows in both broadcast and deliver, then  modify the Deliver API to require explicit ranges (start and end).  Where it would be simple to specify the end as `^uint64(0)` to receive blocks indefinitely.   This seems easier for a client to implement, and assuming HTTP2\/gRPC handles the windowing for us, more efficient than the API as it stands.",
        "ts": "1481600078.000572"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "(Will continue to think on this though)",
        "ts": "1481600093.000573"
    }
]
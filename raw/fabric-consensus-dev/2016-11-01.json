[
    {
        "user": "U2CSGKHRS",
        "text": "<@U2CSGKHRS|teddy> has joined the channel",
        "type": "message",
        "subtype": "channel_join",
        "ts": "1477994033.003012"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "plz review <https:\/\/gerrit.hyperledger.org\/r\/#\/c\/2117\/>",
        "ts": "1477994453.003013"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "<@U0XPR4NP4> <@U0XR6J961> i noticed that orderer.yaml defaults the listening port to `5151`, but the docker-compose overrides it to `5005`, is there a reason for this? why not decide on one of them and go with it everywhere? from SDK point of view we\u2019d like to decide on which one to use for the test cases. right now our tests are set up with docker-compose so it makes sense to use 5005. but having 5151 as default can break contributors who fire up SOLO orderer as native process.",
        "ts": "1478014218.003014"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U13TWBRBL> As I recall, 5005 was picked initially and arbitrarily as a stand-in, but someone, I think <@U0KM61BCP> pointed out that this conflicted with another service we use and asked for it to be changed.  So, when the configuration went in, it was changed to 5151.  I'm not really sure what process we use for picking ports, but I agree, we should pick a port and fix it in both places.",
        "ts": "1478014328.003015",
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "U13TWBRBL"
                ],
                "count": 1
            }
        ]
    },
    {
        "type": "message",
        "user": "U0ZJZBJLF",
        "text": "Is block validation coded at the moment?",
        "ts": "1478014355.003016"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "<https:\/\/gerrit.hyperledger.org\/r\/#\/c\/747\/3\/orderer\/sample_clients\/deliver_stdout\/client.go@104>",
        "ts": "1478014366.003017",
        "reactions": [
            {
                "name": "ok_hand",
                "users": [
                    "U13TWBRBL"
                ],
                "count": 1
            }
        ]
    },
    {
        "type": "message",
        "user": "U0ZJZBJLF",
        "text": "(does anyone here know, or know who writes that aspect?)",
        "ts": "1478014380.003018"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "should i file a JIRA bug for this change?",
        "ts": "1478014399.003019"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "<@U0XPR4NP4> I chose from the list of assigned ports a range of 10 that were unassigned 7050-7060",
        "ts": "1478014418.003020"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "<@U13TWBRBL> Yes please.",
        "ts": "1478014444.003021"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "I asked a while back but did not get an answer as to why we chose 50xx range because those are taken",
        "ts": "1478014457.003022"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "yes, we should use from the set I identified earlier",
        "ts": "1478014471.003023"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "I don\u2019t know why we would use another range",
        "ts": "1478014482.003024"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "when all is said and done we can allocate the range we are using",
        "ts": "1478014494.003025"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Great, let's move to that range, I didn't understand 7050-7060 was what was chosen",
        "ts": "1478014502.003026"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "What port specifically should the orderer use? I assume we have already allocated some of 7050-7060 for some services?",
        "edited": {
            "user": "U0XPR4NP4",
            "ts": "1478014536.000000"
        },
        "ts": "1478014528.003027"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Are other components using ports within that range?",
        "ts": "1478014532.003028"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Oh damn it, too slow.",
        "ts": "1478014537.003030"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "well, a bunch are used, yes",
        "ts": "1478014545.003031"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "<https:\/\/jira.hyperledger.org\/browse\/FAB-923>",
        "ts": "1478014546.003032"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Are these listed somewhere?",
        "ts": "1478014558.003033"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "but with the refactor, we need to assess what is needed going forward",
        "ts": "1478014563.003034"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "eg REST endpoint is deprecated, no?",
        "ts": "1478014574.003035"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "in the PR where I changed things;-)",
        "ts": "1478014588.003036"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "REST APIs are already stripped out of master",
        "ts": "1478014602.003037"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "so 7050 are free now",
        "ts": "1478014609.003038"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "Well, it'd be great if you can link to that PR in Jim's issue.",
        "ts": "1478014617.003039"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "7051 is peer grpc, 7053 is peer\u2019s event stream, 7054 right now is member service grpc",
        "ts": "1478014640.003040"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "<https:\/\/github.com\/hyperledger-archives\/fabric\/pull\/2286>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "[WIP] Fix Issue 2283 by christo4ferris \u00b7 Pull Request #2286 \u00b7 hyperledger-archives\/fabric \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/hyperledger-archives\/fabric\/pull\/2286",
                "text": "Description change peer REST port from 5000 to 7050 to avoid conflict with UPnP on Windows. Also reassigned all other ports as follows: 7050 REST 7051 peer gRPC 7052 peer CLI 7053 peer events 7054 ...",
                "fallback": "GitHub: [WIP] Fix Issue 2283 by christo4ferris \u00b7 Pull Request #2286 \u00b7 hyperledger-archives\/fabric",
                "thumb_url": "https:\/\/avatars1.githubusercontent.com\/u\/3630697?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger-archives\/fabric\/pull\/2286",
                "thumb_width": 400,
                "thumb_height": 400,
                "service_icon": "https:\/\/a.slack-edge.com\/e8ef6\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1478014662.003041"
    },
    {
        "type": "message",
        "user": "U0KM61BCP",
        "text": "so the proliferation of CA endpoints is, I believe no longer desired",
        "ts": "1478014685.003043"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "I updated the issue accordingly, thx.",
        "ts": "1478014729.003044"
    },
    {
        "text": "waits for jenkins to even start",
        "type": "message",
        "subtype": "me_message",
        "user": "U0XR6J961",
        "ts": "1478014748.003045"
    },
    {
        "type": "message",
        "user": "U13TWBRBL",
        "text": "what\u2019s \u201c7052 peer cli\u201d used for? shouldn\u2019t peer cli use \u201c7051\u201d just like SDKs?",
        "ts": "1478014762.003046"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XQ35CDD> <@U0KN2SSKE> Also posting here to prevent bad threading in <#C2GE7GXM1|fabric-crypto> \n\nThe second piece, is how is the authority to create channels tracked, and how is channel creation serialized against this to prevent non-deterministic channel creation while not leaking the channel creation to those who should not witness it. (ie, doing this in the system ledger is probably not an option)",
        "ts": "1478031760.003047"
    },
    {
        "type": "message",
        "user": "U0KN2SSKE",
        "text": "<@U1BC5A0F9> called it \u201csuperuser\u201d, a leader node that processes all config transactions and disseminates to followers",
        "ts": "1478038534.003050"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "<@U0KN2SSKE>: I am not sure this addresses the concern raised here. You ultimately need a log where the configurations specifying who can create a channel, along with the actual requests\/config_txs to create channels, are maintained. And that second part makes the system chain (which was the obvious candidate) not an option because of leaking concerns.",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1478039079.000000"
        },
        "ts": "1478038880.003051"
    },
    {
        "user": "U23DJ7XDZ",
        "text": "<@U23DJ7XDZ|ermyas> has joined the channel",
        "type": "message",
        "subtype": "channel_join",
        "ts": "1478040109.003053"
    }
]
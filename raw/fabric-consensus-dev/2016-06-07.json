[
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "How do I config pbft to be the consensus plugin of peer node?\n\nThis is my config:\n\n```\n    # Validator defines whether this peer is a validating peer or not, and if\n    # it is enabled, what consensus plugin to load\n    validator:\n        enabled: true\n\n        consensus:\n            # Consensus plugin to use. The value is the name of the plugin, e.g. pbft, noops ( this value is case-insensitive)\n            # if the given value is not recognized, we will default to noops\n            plugin: batch\n\n            # total number of consensus messages which will be buffered per connection before delivery is rejected\n            buffersize: 1000\n```\n\nbut I still see the outputs of a `noops` plugin is created:\n\n```\n[36m09:46:45.612 [consensus\/statetransfer] blockThread -&gt; DEBU 02bESC[0m name:\"vp0\"  has validated its blockchain to the genesis block\n09:46:45.612 [consensus\/noops] newNoops -&gt; INFO 02cESC[0m NOOPS consensus type = *noops.Noops\n09:46:45.612 [consensus\/noops] newNoops -&gt; INFO 02dESC[0m NOOPS block size = 500\n09:46:45.612 [consensus\/noops] newNoops -&gt; INFO 02eESC[0m NOOPS block timeout = 1s\n```",
        "edited": {
            "user": "U142E5N0P",
            "ts": "1465293499.000000"
        },
        "ts": "1465293434.000176"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "you set the consensus type in the core.yaml file",
        "ts": "1465293487.000178"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "<@U0XR6J961>:  `plugin: batch` is  configured in the core.yaml file",
        "ts": "1465293533.000180"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "hmm",
        "ts": "1465293540.000181"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ah no, plugin: obcpbft",
        "ts": "1465293566.000182"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i think",
        "ts": "1465293567.000183"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "...",
        "ts": "1465293607.000184"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "`plugin: pbft`",
        "ts": "1465293607.000185"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "<@U142E5N0P>: <https:\/\/github.com\/hyperledger\/fabric\/wiki\/Consensus>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "hyperledger\/fabric",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/wiki\/Consensus",
                "text": "fabric - Fabric is a blockchain project in Incubation proposed to the community and documented at <https:\/\/goo.gl\/RYQZ5N>. Information on what Incubation entails can be found in the Hyperledger Proj...",
                "fallback": "GitHub: hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars3.githubusercontent.com\/u\/7657900?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/wiki\/Consensus",
                "thumb_width": 142,
                "thumb_height": 142,
                "service_icon": "https:\/\/github.com\/apple-touch-icon.png",
                "id": 1
            }
        ],
        "ts": "1465293642.000186"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "<@U0XQ35CDD>:  got it, thank you",
        "ts": "1465293699.000188"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that should definitely be in a comment",
        "ts": "1465294216.000191"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "what options you have",
        "ts": "1465294219.000192"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "When I use `plugin: pbft` and setup the p2p network, I could see each node has the classic pbft plugin worked.  However, when I send a chaincode deploy json to them, why corresponding docker images could not be created anymore?  And I would get a failure when send query request.  This is the output of `docker images`:\n\n```\nREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\nhyperledger-peer               latest              aad903f81518        52 minutes ago      2.066 GB\nhyperledger\/fabric-baseimage   latest              5d4fe4b975c6        4 days ago          1.384 GB\n```\nNo such containers with name of `dev-vp0-7b07c59e9b9405c1aef33493b63b9a766d9bb836989ded1730052de650aa8ce5654274d148ceff96a4e5bd43bca26aba099f55c400e4befdc8b2ee4c0a94e30b`  has ever been created any more.",
        "ts": "1465297235.000193"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i don't know",
        "ts": "1465297319.000194"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "without logs i cannot help",
        "ts": "1465297323.000195"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "<https:\/\/transfer.sh\/z94uL\/peer.dbg.out>",
        "ts": "1465297460.000196"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "are you running 4 nodes?",
        "ts": "1465297624.000197"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "yes",
        "ts": "1465297632.000198"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "this is the output of another node which accept the `deploy` request   <https:\/\/transfer.sh\/GDJ3w\/peer.dbg.out>",
        "ts": "1465297672.000199"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "what version of the code are you running?",
        "ts": "1465297951.000200"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "latest code with commit of `803e594489dcd011970d168403ee328be0b3da3a`",
        "ts": "1465298011.000201"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i don't have that",
        "ts": "1465298165.000202"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ah no",
        "ts": "1465298176.000203"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "weird",
        "ts": "1465298183.000204"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ah, try pbft batch",
        "ts": "1465298228.000205"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "ok",
        "ts": "1465298355.000206"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "still the same.. \nthe config is:\n```\nmode: batch\n```\nin the `consensus\/obcpbft\/config.yaml`,  and the output log is too  huge to be uploaded..  there are many chaotic outputs such as:\n```\n2\\246\\013\\322\\350,D\\321\\251\\350l\\366\\025\\343\\347\\003[\\013\\310\\242\\261\\270\\352t\\023\\335\\255C\\233\\000(\\315YZU\\252\\361\\020\\277\\030\\354\\346\\363$\\240\\266\\334k\\233\\361\\324\\250oW\\307\\204\\200\\\\\\002h\\334;J \\021\\306G\\214\\366\\211^\\261r\\032\\326\\214@\\322o\\\"t\\033\\374X`\\036\\001['\\243\\200sj\\245\\254r\\327B(%\\302\\363(\\263~*F\\331,p\\200\\002\\333+#\\034\\016!2\\217\\030\\311\\352\\010\\261J.\\311\\237\\376\\365\\021\\342\\260p:\\002\\324\\177c\\224s\\t\\013\\220#\\202\\363\\277\\242\\220\\262\\232C\\350F\\306\\003@.\\204\\305Qb\\\n```",
        "edited": {
            "user": "U142E5N0P",
            "ts": "1465299437.000000"
        },
        "ts": "1465299397.000207"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yea i don't know what that is",
        "ts": "1465299955.000210"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "some code paths log content",
        "ts": "1465299967.000211"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "which is unfortunate",
        "ts": "1465299972.000212"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "without log i cannot help at all",
        "ts": "1465299981.000213"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "I cut the head 1800 lines of the log file such that those chaotic outputs are not included: <https:\/\/transfer.sh\/JTA0\/peer.out>   the overall log is above 10G ..    I just send a deploy request",
        "ts": "1465300591.000214"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "your deploy transaction is huge",
        "ts": "1465301169.000215"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "200MB",
        "ts": "1465301174.000216"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and your network needs 5s to send that transaction",
        "ts": "1465301198.000217"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "at that point, consensus decides that something is wrong",
        "ts": "1465301212.000218"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "you will have to adjust your timeouts",
        "ts": "1465301228.000219"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "or reduce your transaction size",
        "ts": "1465301234.000220"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "we have patches in the works that may help on that front",
        "ts": "1465301243.000221"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "(by substantially reducing cruft that makes it into the deploy payload today",
        "ts": "1465301270.000222"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "it's all a hack",
        "ts": "1465301272.000223"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "assuming that is the problem",
        "ts": "1465301273.000224"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "what is?",
        "ts": "1465301278.000225"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "the patches to only include certain types of files",
        "ts": "1465301308.000226"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "well, maybe, but its a stopgap anyway",
        "ts": "1465301326.000227"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "the right way to do it is to ask the chaincode for its list of dependencies, that will go in next",
        "ts": "1465301355.000228"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "but, the hack gets us most of the way there, so if you need a quick fix for the 200MB",
        "ts": "1465301379.000229"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and move the shim dependency out of the main fabric repo",
        "ts": "1465301379.000230"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "that actually doesnt matter any more",
        "ts": "1465301390.000231"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "how does it work then?",
        "ts": "1465301399.000232"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "<@U0XR6J961>  I just deploy the chaincode example2 ..",
        "ts": "1465301410.000233"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yingfeng: probably you have some large log files around",
        "ts": "1465301425.000234"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "<@U0XR6J961>: see the comment about half-way down in PR 1720 <https:\/\/github.com\/hyperledger\/fabric\/pull\/1720>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "Adding filtering on file extension during deploy by angrbrd \u00b7 Pull Request #1720 \u00b7 hyperledger\/fabric \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1720",
                "text": "Description This change is being made to add control as to which files end up inside the .tar.gz package upon deploy. Motivation and Context The original .tar.gz package was going through $GOP...",
                "fallback": "GitHub: Adding filtering on file extension during deploy by angrbrd \u00b7 Pull Request #1720 \u00b7 hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars2.githubusercontent.com\/u\/14612437?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1720",
                "thumb_width": 168,
                "thumb_height": 168,
                "service_icon": "https:\/\/a.slack-edge.com\/e8ef6\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1465301453.000235"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "regarding gofiles.sh",
        "ts": "1465301475.000237"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "there are techniques to result the deps (direct and transitive) for any package",
        "ts": "1465301495.000238"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "and from there, there are techniques to ask any package for the files it includes",
        "ts": "1465301514.000239"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "so, you can simply ask a package such as a chaincode for the complete set of packages\/files it needs, doesnt matter where they live",
        "ts": "1465301538.000240"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "that is the direction both SDK\/NVP for GOLANG and chaintool for CAR need to go",
        "ts": "1465301568.000241"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "(IMO)",
        "ts": "1465301574.000242"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "but in the meantime, the file exclusions work perfectly well, and will substantially reduce the payload size",
        "ts": "1465301594.000243"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ok",
        "ts": "1465301636.000244"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "<@U0XR6J961>\n\nThis is the code directory of my machine <http:\/\/pastebin.com\/km02KA2P>  , I build docker image using this directory, there does not exist a log file here..",
        "attachments": [
            {
                "service_name": "Pastebin",
                "title": "```  136K fabric\/core\/rest  20K fabric\/core\/chaincode\/platforms\/golang  12K fabr - Pastebin.com",
                "title_link": "http:\/\/pastebin.com\/km02KA2P",
                "fallback": "Pastebin: ```  136K fabric\/core\/rest  20K fabric\/core\/chaincode\/platforms\/golang  12K fabr - Pastebin.com",
                "image_url": "http:\/\/pastebin.com\/i\/facebook.png",
                "from_url": "http:\/\/pastebin.com\/km02KA2P",
                "image_width": 250,
                "image_height": 250,
                "image_bytes": 19206,
                "service_icon": "http:\/\/pastebin.com\/favicon.ico",
                "id": 1
            }
        ],
        "ts": "1465301820.000245"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "319M    fabric\/",
        "ts": "1465302068.000247"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "yes, there are some binaries, they are peer and chaincode programs",
        "ts": "1465302237.000248"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yes",
        "ts": "1465302293.000249"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that all gets packaged up",
        "ts": "1465302299.000250"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "<@U142E5N0P>: if you\u2019d like, you can try running on top of the PR 1708 branch and I think that problem will be mitigated",
        "ts": "1465302326.000251"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "1720 is the more direct\/short-term fix, but it needs more work to be ready to work for all cases",
        "ts": "1465302348.000252"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "1708 at least passes CI currently",
        "ts": "1465302358.000253"
    },
    {
        "type": "message",
        "user": "U0KPFAZNF",
        "text": "either way, 1708, 1720, or (most likely) an amalgam will be merged asap",
        "ts": "1465302392.000254"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "got it, thanks",
        "ts": "1465302406.000255"
    },
    {
        "type": "message",
        "user": "U10SJMAHH",
        "text": "<@U0Y14MWA2> I sent you the chaincode by e-mail.",
        "attachments": [
            {
                "fallback": "[June 7th, 2016 6:04 AM] vukolic: but post a notice",
                "author_subname": "vukolic",
                "ts": "1465247091.000168",
                "channel_id": "C0Z4NBUN6",
                "channel_name": "fabric-consensus-dev",
                "is_msg_unfurl": true,
                "text": "but post a notice",
                "author_name": "Marko Vukolic",
                "author_link": "https:\/\/hyperledgerproject.slack.com\/team\/vukolic",
                "author_icon": "https:\/\/secure.gravatar.com\/avatar\/a52edd136bbd07ca069b9393ac60f675.jpg?s=48&d=https%3A%2F%2Fa.slack-edge.com%2F66f9%2Fimg%2Favatars%2Fava_0006-48.png",
                "mrkdwn_in": [
                    "text"
                ],
                "color": "D0D0D0",
                "from_url": "https:\/\/hyperledgerproject.slack.com\/archives\/fabric-consensus-dev\/p1465247091000168",
                "is_share": true,
                "footer": "Posted in #fabric-consensus-dev"
            }
        ],
        "ts": "1465305721.000256"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "hi sachikoy",
        "ts": "1465305756.000257"
    },
    {
        "type": "message",
        "user": "U10SJMAHH",
        "text": "hi",
        "ts": "1465305919.000258"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "do you have debug logs of the failures in 1545, 1331?",
        "ts": "1465306870.000259"
    },
    {
        "type": "message",
        "user": "U10SJMAHH",
        "text": "here is the logs for 1545, for vp0 an vp1  <https:\/\/ibm.box.com\/s\/l9i37p4ex5or44iy3ojbva61d6lzrx43>",
        "ts": "1465308792.000260"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "are these new?",
        "ts": "1465309125.000262"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "because the ones i saw yesterday are incomplete",
        "ts": "1465309169.000263"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "jyellick: <https:\/\/www.readability.com\/articles\/y2qyiyzd>",
        "ts": "1465310647.000264"
    },
    {
        "type": "message",
        "user": "U10SJMAHH",
        "text": "vp1\u2019s log is incomplete because I lost network connection while downloading the log",
        "ts": "1465310709.000265"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961>: Interesting.  I don't think that sells me on entirely on eliminating channels, but it does reaffirm my intuition that channels and mutexes do not mix nicely",
        "ts": "1465311228.000266"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "sachikoy: without full logs, ideally from all peers, we can't really see what is going on",
        "ts": "1465311237.000267"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961> <@U0XQ35CDD> <@U0UGH3X7X> <https:\/\/github.com\/hyperledger\/fabric\/pull\/1744> this is a simple code rename refactor, if you have a chance to glance at and sign off",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "Move obcpbft events infrastructure into its own package by jyellick \u00b7 Pull Request #1744 \u00b7 hyperledger\/fabric \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1744",
                "text": "Description This changeset moves the obcpbft events framework into its own package and fixes all of the corresponding local references in the code to be package references. It also renames the as...",
                "fallback": "GitHub: Move obcpbft events infrastructure into its own package by jyellick \u00b7 Pull Request #1744 \u00b7 hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars0.githubusercontent.com\/u\/7431583?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1744",
                "thumb_width": 420,
                "thumb_height": 420,
                "service_icon": "https:\/\/a.slack-edge.com\/e8ef6\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1465311780.000268"
    },
    {
        "type": "message",
        "user": "U0UGH3X7X",
        "text": "so for #1331,   what should happen when i do an invoke to a peer that's just been re-started ?",
        "ts": "1465313460.000270"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Depends on how 'just' restarted, and the state of the network",
        "ts": "1465315152.000271"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "This all assumes under batch, but if the network has not changed views since the peer went down and back up, then it should forward the request to the current (correct) primary, and it should be ordered by the network",
        "ts": "1465315202.000272"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "This is regardless of the current replica's ability to participate in ordering.",
        "ts": "1465315215.000273"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "If the peer is more out of sync with the network, it may have to wait until it can eavesdrop into the correct watermarks, and for a view change potentially to pick the correct view",
        "ts": "1465315255.000274"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "But eventually, the request should be processed",
        "ts": "1465315267.000275"
    },
    {
        "type": "message",
        "user": "U0UGH3X7X",
        "text": "that's what i expected ... from reading <@U13Q594J2> 's latest logs,  after peer3 is restarted, the subsequent requests are being forwarded and executed but peer3 isn't starting state transfer ...  rechecking now",
        "ts": "1465315526.000276"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "actually when custody runs out, it will broadcast the complaint, and then the system should commit the transaction",
        "ts": "1465315647.000277"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "testing async functions is such a pain",
        "ts": "1465315723.000278"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i don't understand how people usually test these things",
        "ts": "1465315745.000279"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "jyellick: do we ever move watermarks without being up to date?",
        "ts": "1465317097.000280"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Yes",
        "ts": "1465317132.000281"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961> When we first detect we are out of date, we move our watermarks to where the network seems to be operating, this is so that we can collect a weak checkpoint certificate so that we can initiate state transfer.  It also allows us to buffer new transactions while state transfer takes place.",
        "ts": "1465317196.000282"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so at that point we start updating p and q sets",
        "ts": "1465317283.000283"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "(As I've mentioned before, to not grow unboundedly, we only track 1 checkpoint per peer above our watermarks.  Often, we might get a weak cert of matching checkpoints, but it is not guaranteed, we might get f+1 checkpoints for different sequence numbers, or with non-matching hashes, that simply means we are out of date and must listen for a good target)",
        "ts": "1465317315.000284"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Yes, I think that's correct",
        "ts": "1465317335.000285"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so updating lastExec to the watermark is incorrect?",
        "ts": "1465317354.000286"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Right",
        "ts": "1465317390.000287"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Actually, I think that's a bug in the view change code",
        "ts": "1465317416.000288"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "```\n        if instance.lastExec &lt; cp.SequenceNumber {\n                logger.Warning(\"Replica %d missing base checkpoint %d (%s)\", instance.id, cp.SequenceNumber, cp.Id)\n\n                snapshotID, err := base64.StdEncoding.DecodeString(cp.Id)\n                if nil != err {\n                        err = fmt.Errorf(\"Replica %d received a view change who's hash could not be decoded (%s)\", instance.id, cp.Id)\n                        logger.Error(err.Error())\n                        return nil\n                }\n\n                instance.consumer.skipTo(cp.SequenceNumber, snapshotID, replicas)\n                instance.lastExec = cp.SequenceNumber\n        }\n```",
        "ts": "1465317420.000289"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "That `lastExec` should be set once the state transfer completes, not at initiation",
        "ts": "1465317453.000290"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "hmm",
        "ts": "1465317463.000291"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "(It used to be correct, under the old executor model)",
        "ts": "1465317466.000292"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "right",
        "ts": "1465317515.000293"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "because state transfer may sync to a different seqno",
        "ts": "1465317568.000294"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Well, and additionally, I don't see anything there that would keep us from executing transactions before the state transfer completes, (which would temporarily corrupt the blockchain, and actually potentially permanently cause divergence )",
        "edited": {
            "user": "U0XPR4NP4",
            "ts": "1465317657.000000"
        },
        "ts": "1465317623.000295"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but we're set to \"syncing\"",
        "ts": "1465317675.000297"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Where?",
        "ts": "1465317681.000298"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "```\tif instance.skipInProgress {\n\t\tlogger.Debug(\"Replica %d currently picking a starting point to resume, will not execute\", instance.id)\n\t\treturn false\n\t}\n```",
        "ts": "1465317704.000299"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Right, but in the view change, where do we set `instance.skipInProgress`?",
        "ts": "1465317718.000300"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "aha!",
        "ts": "1465317734.000301"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "view change",
        "ts": "1465317737.000302"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "also",
        "ts": "1465317750.000303"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "what happens if we are executing",
        "ts": "1465317759.000304"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and we trigger a state transfer",
        "ts": "1465317782.000305"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "they will execute concurrently",
        "ts": "1465317788.000306"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "That does seem like a potential race",
        "ts": "1465317791.000307"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "This is why the executor combined the two",
        "ts": "1465317803.000308"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "maybe now it is time to build a small executor using the events system",
        "ts": "1465317854.000309"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I wonder if it doesn't belong outside of obcpbft, like state transfer",
        "ts": "1465317877.000310"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and state transfer can probably also unwind by using events",
        "ts": "1465317881.000311"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yea it does",
        "ts": "1465317884.000312"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I am trying to remember, what execution path work was it you were going to tackle?",
        "ts": "1465317988.000313"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "the preview exec + commit on checkpoint?",
        "ts": "1465318064.000314"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Ah, yes, that's right",
        "ts": "1465318086.000315"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Wondering which work should be done first",
        "ts": "1465318159.000316"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "bug fix over features",
        "ts": "1465318269.000317"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Fair enough.  As I know the state transfer semantics well, want me to tackle the mini-executor then?",
        "ts": "1465318422.000318"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and with it the commit on viewchange race?",
        "ts": "1465318761.000319"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Right",
        "ts": "1465318941.000320"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "okay",
        "ts": "1465319281.000321"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961>: Are you still handy?",
        "ts": "1465320194.000322"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i am",
        "ts": "1465320199.000323"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Designing the API for the executor",
        "ts": "1465320209.000324"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Since the executor must necessarily have its own thread for performing executions (and we don't want to block until they complete)",
        "ts": "1465320236.000325"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I was trying to decide how we should pass back the result to a call like `Preview` or `Commit`",
        "ts": "1465320266.000326"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "The most simple\/direct approach is to supply a callback which will be invoked with the return value, but it's not the most intuitive of APIs",
        "ts": "1465320300.000327"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Something like: `Preview(callback func(uint64, *pb.Block))` or `Execute(txs []*pb.Transaction, callback func())`",
        "ts": "1465320326.000328"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Thoughts?",
        "ts": "1465320354.000329"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yea, thinking",
        "ts": "1465320360.000330"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "would it be enough for it to emit an event?",
        "ts": "1465320436.000331"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i suppose for decoupling that would be a callback",
        "ts": "1465320452.000332"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Right, ultimately it will be converted back to an event",
        "ts": "1465320524.000333"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "The other option would be to specify a callback receiver at instantiation",
        "ts": "1465320543.000334"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "That would make the API usage a little more straightforward I would think, though it would reduce the flexibility a bit.",
        "ts": "1465320565.000335"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i think supplying an interface is more idiomatic?",
        "ts": "1465320587.000336"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "on instantiation",
        "ts": "1465320593.000337"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Okay, that's fine with me",
        "ts": "1465320605.000338"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but then you have the problem with concurrent bringup",
        "ts": "1465320610.000339"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "It does make instantiation a little annoying, but, because there's a `Start()`, it should be safe",
        "ts": "1465320651.000340"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i wonder what the idiomatic way is",
        "ts": "1465320752.000341"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "As do I, seems like there must be a better pattern",
        "ts": "1465320888.000342"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961>: Still around?",
        "ts": "1465323235.000343"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yes",
        "ts": "1465323313.000344"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "So, I've tracked down the problem <@U0N1D1UAE> has been having with 'empty' blocks.  Basically, in the execution loop, we mark all of the requests as stale, and end up with a slice of transactions which is 0 length.",
        "ts": "1465323453.000345"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "(In this case, there was only 1 transaction to begin with)",
        "ts": "1465323488.000346"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "oh",
        "ts": "1465323501.000347"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "how come",
        "ts": "1465323523.000348"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "So, I'm wondering two things, one, what should we do if there are no valid transactions in a batch? I would think we should not write a block.",
        "ts": "1465323531.000349"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "why did the primary include stale requests?",
        "ts": "1465323551.000350"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "And secondly, it seems like the primary including stale transactions is a bug",
        "ts": "1465323555.000351"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "probably should trigger view change",
        "ts": "1465323561.000352"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Right, as it would indicate byzantine behavior",
        "ts": "1465323578.000353"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "primary should sort requests by the same sender",
        "ts": "1465323619.000354"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "So, I think the way this is happening...",
        "ts": "1465323631.000355"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Is that the primary is receiving requests from REST with out of order timestamps",
        "ts": "1465323647.000356"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Because if there are concurrent requests to be delivered into consensus, it is not FIFO, it is pseudorandom (per standard channel writer behavior)",
        "ts": "1465323682.000357"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Or wait, maybe not...",
        "ts": "1465323728.000358"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "We make a new timestamp it looks like",
        "ts": "1465323740.000359"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yes we do",
        "ts": "1465325034.000360"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "The leader is receiving stale requests from itself",
        "ts": "1465325410.000361"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "hi guys - i had this open and was reading",
        "ts": "1465325580.000362"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "FIFO is not required for BFT... but if it is missing, clients often wonder what happens",
        "ts": "1465325609.000363"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "and even write papers .... <https:\/\/arxiv.org\/abs\/1605.05438> ...",
        "ts": "1465325638.000364"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "so if you can support it without much cost, then it makes a lot of sense",
        "ts": "1465325658.000365"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "hi cca",
        "ts": "1465325671.000366"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I think even if we were to not use the psuedorandom channel writer stuff, FIFO would still be difficult to promise, as requests can come in concurrently",
        "ts": "1465325891.000367"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "cca: i think these guys didn't read the nakamoto paper?",
        "ts": "1465325984.000368"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "just because a block has been mined doesn't mean that the data should be considered \"committed\"",
        "ts": "1465326030.000369"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "FIFO is defined as per-sender order",
        "ts": "1465326364.000370"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "can be implemented in the obvious way, with a sequence number stored at the sender (without a concept of a sender, it isn't defined)",
        "ts": "1465326391.000371"
    },
    {
        "type": "message",
        "user": "U0XV1HDL3",
        "text": "simon: they go into the depth of the chain, and parameterize what is decided by the depth. you have to make some choice like this, in nakamoto consensus -- otherwise, if i buy my house using bitcoin, then the chain reverts and forks to something else, do i have to move out?",
        "ts": "1465326528.000372"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961>: Are you seeing how the primary could be submitting stale requests to itself?",
        "ts": "1465327039.000373"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Aha! Found it",
        "ts": "1465328039.000374"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "re commit on checkpoints as 1545 turns out non-deterministic and 1701 bogus use of noops - I am more convinced that we should not do it",
        "ts": "1465329685.000375"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "there is no use in masking non-determinism (sometimes)",
        "ts": "1465329694.000376"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "that said - we desperately need to help chaincode developers not write non-deterministic chaincode - and I am not aware that anybody is looking into how to do this",
        "ts": "1465329853.000377"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0Y14MWA2>: I think the converse argument would be that committing on checkpoint would generally reveal nondeterminism more immediately, the blockchain should halt in a consistent state, and the set of trans which caused the non-determinism would be obvious",
        "ts": "1465330239.000378"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "actually it wouldn't",
        "ts": "1465333326.000379"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "if non determinism appears in one peer then it could be masked",
        "ts": "1465333346.000380"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "anyway - for the record - I am not in support of that",
        "ts": "1465333365.000381"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "one can have at a checkpoint detection that we diverged from others",
        "ts": "1465333442.000382"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "and shut down the machine - that would be ok",
        "ts": "1465333449.000383"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "I am more in favor of debugging Sieve and\/or moving to Cons v2 to address non-determinism",
        "ts": "1465333536.000384"
    },
    {
        "type": "message",
        "user": "U0Y14MWA2",
        "text": "addressing it incompletely is not satisfactory IMO",
        "ts": "1465333553.000385"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Yes, for non-determinism which is exhibited in less than f peers, it would not be detected, but under  byzantine conditions, I believe it is provably not solvable (including under Sieve).",
        "ts": "1465356042.000386"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961> <@U0UGH3X7X> <@U0XQ35CDD> <https:\/\/github.com\/hyperledger\/fabric\/pull\/1749>\n\nThis should fix the empty blocks that <@U0N1D1UAE> has been observing under batch.  Essentially, when `pbft-core.go` runs out of sequence numbers, it begins buffering requests in a map, and when the watermarks move, it resubmits the requests in map iterator order (which is effectively random).  Because the deduplicator filters out 'old' requests, we end up with blocks which contain no transactions (and end up abandoning requests which we should not).",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "Correctly order PBFT requests on resubmission by jyellick \u00b7 Pull Request #1749 \u00b7 hyperledger\/fabric \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1749",
                "text": "Description This changeset causes the resubmission of requests to occur in order by timestamp, rather than randomly in map order. This changeset also includes a couple minor lint fixes and some e...",
                "fallback": "GitHub: Correctly order PBFT requests on resubmission by jyellick \u00b7 Pull Request #1749 \u00b7 hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars0.githubusercontent.com\/u\/7431583?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1749",
                "thumb_width": 420,
                "thumb_height": 420,
                "service_icon": "https:\/\/h2.slack-edge.com\/e8ef6\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1465356226.000387"
    },
    {
        "type": "message",
        "user": "U142E5N0P",
        "text": "<@U0KPFAZNF>:  I've applied the patch of PR 1708, and now, on each machine, here are the results of `docker images`:\n```\nREPOSITORY                     TAG                 IMAGE ID            CREATED             SIZE\nhyperledger\/fabric-peer        latest              820b0b3235d8        21 minutes ago      1.443 GB\nhyperledger\/fabric-ccenv       latest              87324f87a686        21 minutes ago      1.433 GB\nhyperledger\/fabric-src         latest              f68c00309ee5        21 minutes ago      1.416 GB\nhyperledger\/fabric-baseimage   latest              43574ff03f31        43 minutes ago      1.384 GB\n```\n\nNow I start 4 peer nodes on 4 different machines, with the consensus configuration of pbft:\n`plugin:pbft` in `peer\/core.yaml` and `mode batch` in `consensus\/obcpbft\/config.yaml`\n\nThe deployment behavior is different from before---it returns immediately, I still use the chaincode example2 as the chaincode deployment test.\n\nHowever, after deployment, I could not get successful query:\n```\ncurl -H \"Content-Type: application\/json\" -X POST --data \"@query.json\"  -k <http:\/\/192.168.0.147:5000\/chaincode>\n{\"jsonrpc\":\"2.0\",\"error\":{\"code\":-32003,\"message\":\"Query failure\",\"data\":\"Error when querying chaincode: Error:Failed to launch chaincode spec(Could not get deployment transaction for 7b07c59e9b9405c1aef33493b63b9a766d9bb836989ded1730052de650aa8ce5654274d148ceff96a4e5bd43bca26aba099f55c400e4befdc8b2ee4c0a94e30b - LedgerError - ResourceNotFound: ledger: resource not found)\"},\"id\":5}\n```\n\nThe logs of 4 machines are uploaded here: <https:\/\/transfer.sh\/15kVnV\/logs.tar.gz>",
        "ts": "1465361214.000389"
    }
]
[
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "unfortunately my behave doesn't work well for 1874",
        "ts": "1466509329.000326"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i'm having problems with deploy",
        "ts": "1466509348.000327"
    },
    {
        "type": "message",
        "user": "U0UGH3X7X",
        "text": "gimme a few minutes  ... doing a vagrant destroy\/up  ... will get the debug logs for the @issue_1874 tests",
        "ts": "1466511966.000328"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "why can't i deploy chaincode? puzzling, puzzling",
        "ts": "1466513451.000329"
    },
    {
        "type": "message",
        "user": "U0UGH3X7X",
        "text": "what's the error on deploy ?  is it a timeout again ?",
        "ts": "1466514202.000330"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "no error",
        "ts": "1466514624.000331"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "like it never finishes",
        "ts": "1466514629.000332"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but not even a timeout",
        "ts": "1466514633.000333"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ah now a timeout",
        "ts": "1466514647.000334"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "2 minutes timeout",
        "ts": "1466514662.000335"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "also there is no container running",
        "ts": "1466514683.000336"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "is that some weird non-vagrant thing again?",
        "ts": "1466514691.000337"
    },
    {
        "type": "message",
        "user": "U0UGH3X7X",
        "text": "maybe   ... i'm having to do vagrant destroy because when i tried make behave-deps earlier, i got \"no permissions to install\"",
        "ts": "1466514783.000338"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "<@U0UGH3X7X>, that may be an issue with the installation of grpcio package for python",
        "ts": "1466516031.000339"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yea i installed that",
        "ts": "1466516120.000340"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "it's really that the chaincode subsystem can't start a container, it seems",
        "ts": "1466516136.000341"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "<@U0XR6J961> do you notice a new image downloaded?",
        "ts": "1466516566.000342"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "see in <#C0YPYBVJM>",
        "ts": "1466516588.000343"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "doesn't seem to be able to connect to the docker process",
        "ts": "1466516599.000344"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "wondering if a base image is being downloaded and the chaincode container is taking a while to build?",
        "ts": "1466516664.000345"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "does 'docker images' show new age on any of the base images?",
        "ts": "1466516680.000346"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "no, nothing is being downloaded",
        "ts": "1466516946.000347"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "jyellick: doesn't seem to be a consensus issue?",
        "ts": "1466519340.000348"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "<@U0UKTPMG8>: could you have a look at 1874 - seems peers are not automatically reconnecting to each other?",
        "ts": "1466519383.000349"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961>: Was putting together the final changes for 1928, haven't had a chance to look at the logs yet",
        "ts": "1466519409.000350"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i.e. if the rootnode bounces, it cannot join back (and probably other nodes can't join the network either)",
        "ts": "1466519412.000351"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "okay",
        "ts": "1466519414.000352"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "vp0 comes up, and nobody ever connects to it, nor does it connect to other nodes",
        "ts": "1466519436.000353"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i think that goes back to the lack of a fixed peer list we should have",
        "ts": "1466519447.000354"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Is this the 'root discovery node' thing?",
        "ts": "1466519570.000355"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i think this is it",
        "ts": "1466519673.000356"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "we probably would have to persist a list of nodes\/ips that we ever saw, and try to connect to any of those",
        "ts": "1466519700.000357"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but that's in the peer - consensus can't do anything about it",
        "ts": "1466519719.000358"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "I think there might be an easier fix",
        "ts": "1466519929.000359"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "have the rest of the nodes attempt to reconnect to the rootnode if they're not connected to it",
        "ts": "1466520077.000360"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "<@U0XR6J961> seems to pass with latest master.  I touched base with <@U1B5DPRLG>, she will contact me if any more issues",
        "ts": "1466520094.000361"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "so invoke this line (if the rootNode is not in your peersList) <https:\/\/github.com\/hyperledger\/fabric\/blob\/master\/core\/peer\/peer.go#L537>",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1466520133.000000"
        },
        "ts": "1466520094.000362"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "fix, as in \"stopgap measure\"",
        "ts": "1466520151.000366"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "jeffgarratt: well, it fails here with latest master",
        "ts": "1466520153.000367"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "hmmm",
        "ts": "1466520161.000368"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "there is a new ensureConnected function that runs in background",
        "ts": "1466520172.000369"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i don't know why",
        "ts": "1466520183.000370"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "also imagine: vp0 stays down",
        "ts": "1466520194.000371"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "how does vp3 connect back to vp1 and vp2?",
        "ts": "1466520205.000372"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "if they were NOT given a rootnode list, they would not, unless retrieved from discovery from remaining",
        "ts": "1466520233.000373"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ah that logic doesn't work",
        "ts": "1466520259.000374"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "see, the problem is that vp1 and vp2 are connected with each other",
        "ts": "1466520269.000375"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so len(peersMsg.Peers) &gt; 0",
        "ts": "1466520283.000376"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "which is why I suggest: <https:\/\/hyperledgerproject.slack.com\/archives\/fabric-consensus-dev\/p1466520094000362>",
        "attachments": [
            {
                "from_url": "https:\/\/hyperledgerproject.slack.com\/archives\/fabric-consensus-dev\/p1466520094000362",
                "fallback": "[June 21st, 2016 7:41 AM] kostas: so invoke this line (if the rootNode is not in your peersList) <https:\/\/github.com\/hyperledger\/fabric\/blob\/master\/core\/peer\/peer.go#L537>",
                "author_subname": "kostas",
                "ts": "1466520094.000362",
                "channel_id": "C0Z4NBUN6",
                "channel_name": "fabric-consensus-dev",
                "is_msg_unfurl": true,
                "text": "so invoke this line (if the rootNode is not in your peersList) <https:\/\/github.com\/hyperledger\/fabric\/blob\/master\/core\/peer\/peer.go#L537>",
                "author_name": "Kostas Christidis",
                "author_link": "https:\/\/hyperledgerproject.slack.com\/team\/kostas",
                "author_icon": "https:\/\/avatars.slack-edge.com\/2016-04-05\/31983107923_80db5353e9278df980c7_48.png",
                "mrkdwn_in": [
                    "text"
                ],
                "id": 1,
                "footer": "Posted in #fabric-consensus-dev"
            }
        ],
        "ts": "1466520306.000377"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so do we just have to give all peers a full rootnode list?",
        "ts": "1466520314.000379"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "kostas: what if the rootnode is down?",
        "ts": "1466520337.000380"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "how does vp3 connect to vp1 and vp2?",
        "ts": "1466520344.000381"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "if the answer is, populate the rootnode list with all validators, fine",
        "ts": "1466520372.000382"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that's perfect",
        "ts": "1466520374.000383"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "if this is something we can do",
        "ts": "1466520434.000384"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "I see your point, with vp0 down, if vp1 also goes down and comes back up, vp3 won't ever be able to reconnect to it",
        "edited": {
            "user": "U0XQ35CDD",
            "ts": "1466520462.000000"
        },
        "ts": "1466520446.000385"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "correct",
        "ts": "1466520456.000386"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "we would need to add some sort of recently connected logic with retry",
        "ts": "1466520470.000388"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "which would be fairly simple to do",
        "ts": "1466520478.000389"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "jeffgarratt: can we have more than 1 node in the root list?",
        "ts": "1466520483.000390"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "but the rootNode list would work for now",
        "ts": "1466520487.000391"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "yes",
        "ts": "1466520488.000392"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "it's an array, yes",
        "ts": "1466520492.000393"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and the peer tries to keep connections to all of them?",
        "ts": "1466520497.000394"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "no, only if totally lost conns",
        "ts": "1466520506.000395"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "oh",
        "ts": "1466520510.000396"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that's bad",
        "ts": "1466520512.000397"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "could add that fairly easily",
        "ts": "1466520518.000398"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "because it means that we can have a partitioned network",
        "ts": "1466520521.000399"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "we can definitely add more intelligence to the 'maintained' connections",
        "ts": "1466520548.000400"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i think if we can try to keep a connection to every node in the rootnodes, we're good",
        "ts": "1466520573.000401"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "say isntead of Peers ==0, Peers &lt; len(rootNodes)",
        "ts": "1466520580.000402"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yep",
        "ts": "1466520584.000403"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "well",
        "ts": "1466520586.000404"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "yeah",
        "ts": "1466520587.000405"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and filter yourself from rootnodes",
        "ts": "1466520591.000406"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "then all nodes can have the same config",
        "ts": "1466520597.000407"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "simple enough",
        "ts": "1466520623.000408"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "okay, should i add to the issue that you're on it?",
        "ts": "1466520653.000409"
    },
    {
        "type": "message",
        "user": "U0UKTPMG8",
        "text": "sure thing",
        "ts": "1466520672.000410"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "okay, this will have to go into release and master",
        "ts": "1466520691.000411"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "thanks!",
        "ts": "1466520730.000412"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "okay",
        "ts": "1466520932.000413"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961> <https:\/\/github.com\/hyperledger\/fabric\/pull\/1938> this incorporates the timeout you requested if you could take a look",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "Ensure message order of broadcasts by jyellick \u00b7 Pull Request #1938 \u00b7 hyperledger\/fabric \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1938",
                "text": "Description This changeset modifies the obcpbft broadcast.go to always send messages in the order in which they were received. This is a companion to PR#1928 and incorporates some feedback from @...",
                "fallback": "GitHub: Ensure message order of broadcasts by jyellick \u00b7 Pull Request #1938 \u00b7 hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars0.githubusercontent.com\/u\/7431583?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/pull\/1938",
                "thumb_width": 420,
                "thumb_height": 420,
                "service_icon": "https:\/\/a.slack-edge.com\/e8ef6\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1466521419.000414"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XQ35CDD> <@U0XR6J961> <@U0UGH3X7X> Even with the broadcast ordering fix (which helps), I'm still seeing the occasional duplicated request slip through under busywork.\n\nWhat is happening with vp0 as primary and vp1 receiving a request, is:\n\nvp1 broadcasts request A to network\nvp0,vp2 receives request A from vp1\nvp0 sends pre-prepare to network\nNetwork prepares and commits request A, including vp3\nFinally, vp3 receives broadcast of request A, which adds it to outstanding requests, and eventually gets it executed",
        "ts": "1466521583.000416"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I asked <@U0TFEHX8E> about querying the ledger for existing transaction UUIDs before ordering, but that is a DB hit, which means it may require disk IO, and is probably not something we want in our consensus path if we can avoid it.",
        "ts": "1466521710.000417"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I think it wouldn't be too difficult to hook the deduplicator code from <@U0XR6J961> which is still sitting there, to try to squash this.  I think it opens up the censorship window again, but not executing a transaction seems better than executing it twice.",
        "ts": "1466521786.000418"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "This is something that I've only seen under the high load of busywork, and obviously requires some odd network latencies.  What does everyone think about what approach should be taken, and whether this is critical for 0.5",
        "ts": "1466521865.000419"
    },
    {
        "type": "message",
        "user": "U0XQ35CDD",
        "text": "if we keep track of the last X (10? 50?) requests executed, would that mitigate the problem? and if the answer is yes, can we  store these requests in a data structure without O(n) search times? (which I think was Simon's original concern)",
        "ts": "1466522184.000420"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "who needs to deduplicate, that's the question",
        "ts": "1466522522.000421"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "So, I did implement that list last night as a \"will this work\", and the short answer is \"generally yes\".  The thing I dislike about it though, is that you're basically just making an unlikely event less likely, not completely eliminating it.  Maybe that's good enough, especially as ultimately, it seems like the chaincode\/crypto layer will need to defend against more malicious replay.",
        "edited": {
            "user": "U0XPR4NP4",
            "ts": "1466522553.000000"
        },
        "ts": "1466522539.000422"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and i think the answer is: everybody, and during pre-prepare",
        "ts": "1466522548.000423"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and this is something that security needs to do",
        "ts": "1466522584.000425"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "because these occasional duplications right now are because of our code",
        "ts": "1466522612.000426"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but any byzantine actor could resubmit requests at will",
        "ts": "1466522631.000427"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Right.  We could certainly check the DB before accepting any request for ordering (either accepting a pre-prepare, or sending one), but obviously that slows us down, and as we talk of splitting the consensus network from the endorsers and actual ledger, that becomes even more expensive.",
        "ts": "1466522727.000428"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "we really need to fuse batch and core",
        "ts": "1466522730.000429"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Agreed",
        "ts": "1466522739.000430"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "how does pbft solve this?",
        "ts": "1466522799.000431"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I think PBFT assumes that broadcasts are atomic",
        "ts": "1466522816.000432"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Or, at least that messages arrive in the order in which they were sent",
        "ts": "1466522845.000433"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "(across nodes)",
        "ts": "1466522853.000434"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so how come vp3 receives them in a different order?",
        "ts": "1466522912.000435"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "the broadcaster issue?",
        "ts": "1466522919.000436"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Before the patch to the broadcaster, it was much more common",
        "ts": "1466522969.000437"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so why is it still possible at all?",
        "ts": "1466523023.000438"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "The promise we get is that: \"Messages sent by one node to another will arrive in the order they were sent\", and my suspicion is that PBFT is assuming \"A message sent in response to a broadcast, will arrive after the broadcast has been received by all nodes\"",
        "ts": "1466523048.000439"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "This is possible, because vp3 can commit a request without receiving any messages from vp1",
        "ts": "1466523090.000440"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "well that is silly",
        "ts": "1466523093.000441"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that assumption",
        "ts": "1466523104.000442"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Maybe that assumption isn't really there, I am looking at the paper now",
        "ts": "1466523126.000443"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "oh!",
        "ts": "1466523148.000444"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ha!",
        "ts": "1466523149.000445"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "or not?",
        "ts": "1466523153.000446"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "does the primary include the request in the pre-prepare?",
        "ts": "1466523166.000447"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "i think it does",
        "ts": "1466523185.000448"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and we did that, because otherwise replicas would discard pre-prepares if they didn't have a matching request",
        "ts": "1466523218.000449"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "It may be the timestamp deduplication",
        "ts": "1466523264.000450"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Ah, yes, that's true",
        "ts": "1466523276.000451"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so if we take that out",
        "ts": "1466523287.000452"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "then vp3 would discard pre-prepare, and immediatelly get left behind",
        "ts": "1466523316.000453"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that's silly",
        "ts": "1466523318.000454"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so we'd have to queue messages",
        "ts": "1466523327.000455"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Yes, we would be trading one problem for another",
        "ts": "1466523329.000456"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "and possibly fetch the request",
        "ts": "1466523339.000457"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but it would reduce the amount of work the primary has to do",
        "ts": "1466523357.000458"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "well, network IO",
        "ts": "1466523385.000459"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "completely impossible to reason about this",
        "ts": "1466523394.000460"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "But this doesn't really solve the duplication issue.  I think that it's got to be the timestamping.  PBFT wants the client to broadcast requests with incrementing timestamps",
        "ts": "1466523405.000461"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yes",
        "ts": "1466523411.000462"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "but then request can get lost",
        "ts": "1466523421.000463"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Yes",
        "ts": "1466523434.000464"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "if they are submitted concurrently",
        "ts": "1466523435.000465"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Exactly, or, the byzantine primary can always pick the highest timestamp to order first, censoring all previous requests",
        "ts": "1466523467.000466"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yes",
        "ts": "1466523476.000467"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "\"Additionally, replicas need to remember the 8-byte timestamp of the last request executed by each client to  ensure exactly once semantics. But since timestamps are small and timestamps of inactive clients can be stored on disk, this should not cause a significant scalability problem.\"",
        "ts": "1466523603.000468"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "I guess the idea is that clients are supposed to wait for a reply before submitting a new request",
        "ts": "1466523618.000469"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "It seems like that would lower throughput, but we could certainly implement it as such.",
        "ts": "1466523639.000470"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "yea",
        "ts": "1466524055.000471"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "that's what i had",
        "ts": "1466524058.000472"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "ah, executed",
        "ts": "1466524074.000473"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so that would have to come from the ledger then",
        "ts": "1466524083.000474"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "because on state transfer, consensus loses that information",
        "ts": "1466524098.000475"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "Yes",
        "ts": "1466524111.000476"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "so i think we should declare this a problem of some other part of the stack",
        "ts": "1466524114.000477"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "which it is.",
        "ts": "1466524118.000478"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "if somebody before consensus replays the transaction, we can't do anything about it",
        "ts": "1466524143.000479"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "So I think the reply you'll here, is that \"we should not order transactions which are expected to fail\"",
        "ts": "1466524222.000480"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "But I think if we ever want to achieve truly high throughput, we really need to not perform a ton of introspection on requests.  I would reply with \"Who cares about the validity of what comes out of consensus, so long as everyone agrees on the contents and the order\"",
        "ts": "1466524336.000481"
    },
    {
        "type": "message",
        "user": "U0XR6J961",
        "text": "hm",
        "ts": "1466524370.000482"
    },
    {
        "type": "message",
        "user": "U0XPR4NP4",
        "text": "<@U0XR6J961>: Did you have a chance to review the broadcast changes?",
        "ts": "1466524824.000483"
    }
]
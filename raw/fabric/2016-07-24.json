[
    {
        "user": "U1N2L5UNT",
        "text": "<@U1N2L5UNT|kurokuro> has joined the channel",
        "type": "message",
        "subtype": "channel_join",
        "ts": "1469405107.000337"
    },
    {
        "type": "message",
        "user": "U1JBVRNQ3",
        "text": "Hey guys",
        "ts": "1469419741.000342"
    },
    {
        "type": "message",
        "user": "U1JBVRNQ3",
        "text": "We have several institutions that want to interact with each other and maintain a common ledger. Each transaction that is initiated by a party contains information of that party and another party involved in the chain. We do not want the transaction to reach a third institution that has no connection with the transaction. The intention is to avoid notifying a party that a transaction happened when the transaction is not relevant to the party. The transaction should be seen by only the 2 parties that are involved in it. Confidentiality of the transaction is not the objective of doing this. We want to avoid other parties spending computational resources on the transaction.\nIf the result of the computation reaches all peers, we don\u2019t mind. But the cause of the change should not make it to peers other than the ones involved in the transaction.\n\nAt the transaction level, is it possible to specify the peers that will be made aware of the transaction. It is acceptable that the result of the transaction will be made available to all peers.",
        "ts": "1469419900.000343"
    },
    {
        "user": "U1422GVM1",
        "text": "<@U1422GVM1|yajneshrai> has joined the channel",
        "type": "message",
        "subtype": "channel_join",
        "ts": "1469420191.000344"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "<@U1JBVRNQ3>: I'm not sure how much of what you're asking for is available in pbft consensus, but you might want to read this: <https:\/\/github.com\/hyperledger\/fabric\/wiki\/Next-Consensus-Architecture-Proposal>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "hyperledger\/fabric",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/wiki\/Next-Consensus-Architecture-Proposal",
                "text": "fabric - Fabric is a blockchain project in Incubation proposed to the community and documented at <https:\/\/goo.gl\/RYQZ5N>. Information on what Incubation entails can be found in the Hyperledger Proj...",
                "fallback": "GitHub: hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars3.githubusercontent.com\/u\/7657900?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/wiki\/Next-Consensus-Architecture-Proposal",
                "thumb_width": 142,
                "thumb_height": 142,
                "service_icon": "https:\/\/github.com\/apple-touch-icon.png",
                "id": 1
            }
        ],
        "ts": "1469423906.000346"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "does anyone here know if peers are meant to recover if they \"miss\" a transaction due to being offline?\n\nMy scenario is vp0 thru vp3 running pbft. Then vp0 goes offline (docker stop vp0), while a transactions occurs on another peer. Then vp0 comes back up (docker start vp0) and it never seems to learn of the transaction that happened while down. Any transactions on vp0 is never replicated to the other peers and the same goes for the other peers transactions not replicating to vp0.\n\nI just end up with repeated logs like this:\n`05:24:45.151 [consensus\/pbft] sendViewChange -&gt; INFO bac Replica 0 sending view-change, v:1, h:0, |C|:1, |P|:2, |Q|:3`\n`05:24:45.152 [consensus\/pbft] recvViewChange -&gt; INFO bad Replica 0 received view-change from replica 0, v:1, h:0, |C|:1, |P|:2, |Q|:3`\n`05:24:45.152 [consensus\/pbft] recvViewChange -&gt; WARN bae Replica 0 already has a view change message for view 1 from replica 0`",
        "edited": {
            "user": "U1LES78TT",
            "ts": "1469424352.000000"
        },
        "ts": "1469424319.000348"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "<@U1LES78TT>: if i remember correctly, that depends on the consensus setup.",
        "ts": "1469429486.000352"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "and there is some formula in case of pbft, on the basis of which a lagging node catches up..",
        "ts": "1469429514.000353"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "i mean the catchup is triggered only if that formula's condition is satisfied.",
        "ts": "1469429552.000354"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "ok... I'm running batch pbft with the default settings\nI came across this issue as well which seems very relevant\n\n<https:\/\/github.com\/hyperledger\/fabric\/issues\/1331>",
        "attachments": [
            {
                "service_name": "GitHub",
                "title": "ledger data become inconsistent across nodes after restarting a peer (using PBFT batch) \u00b7 Issue #1331 \u00b7 hyperledger\/fabric \u00b7 GitHub",
                "title_link": "https:\/\/github.com\/hyperledger\/fabric\/issues\/1331",
                "text": "I am testing on an environment with 4 VPs (vp0, .. vp3) with PBFT batch and security enabled. When I stop one peer and restart it, the peer's state becomes inconsistent with other peers, yet all pe...",
                "fallback": "GitHub: ledger data become inconsistent across nodes after restarting a peer (using PBFT batch) \u00b7 Issue #1331 \u00b7 hyperledger\/fabric",
                "thumb_url": "https:\/\/avatars0.githubusercontent.com\/u\/15377192?v=3&s=400",
                "from_url": "https:\/\/github.com\/hyperledger\/fabric\/issues\/1331",
                "thumb_width": 125,
                "thumb_height": 125,
                "service_icon": "https:\/\/a.slack-edge.com\/e8ef6\/img\/unfurl_icons\/github.png",
                "id": 1
            }
        ],
        "ts": "1469429632.000355"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "currently running 500-ish transactions to see if it makes my lagging node catch up",
        "ts": "1469429654.000357"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "YEs,",
        "ts": "1469429678.000358"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "thsi issue was even what i was looking to point you to :_",
        "ts": "1469429696.000359"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": ":slightly_smiling_face:",
        "ts": "1469429698.000360"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "haha, I can't even query my laggard peer now...\n\nError: Error querying chaincode: rpc error: code = 2 desc = \"Error: state may be inconsistent, cannot query\"",
        "ts": "1469429724.000361"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "there somewhere towards the end, this formula of 2f+1 or something was quoted.",
        "ts": "1469429729.000362"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "oh man..  :wink:",
        "ts": "1469429755.000363"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "yeah ok, you mean how pbft tolerates (n-1)\/3 failures in an 'n' node network",
        "ts": "1469429803.000364"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "oh yea , sorry my bad.. my memory with mathematical stuff is horrible :slightly_smiling_face:",
        "ts": "1469429822.000365"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "I didn't think it was meant to leave the failing nodes permanently behind though",
        "ts": "1469429842.000366"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "there's nothing wrong with my laggard peer, it just missed one transaction while it was offline",
        "ts": "1469429866.000367"
    },
    {
        "type": "message",
        "user": "U1CS08EP3",
        "text": "but it also said, that the catch up will start once the number of proper nodes is below some number..",
        "ts": "1469429904.000368"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "yeah I'm doing the same to a second node now... to see if it helps",
        "ts": "1469429981.000369"
    },
    {
        "type": "message",
        "user": "U1LES78TT",
        "text": "2\/4 should not be tolerated",
        "ts": "1469429987.000370"
    }
]